{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4981137a",
   "metadata": {},
   "source": [
    "# Why care about optimization theory at all?\n",
    "## An introduction to Optimization Theory\n",
    "### By Frederick \"Forrest\" Miller\n",
    "\n",
    "In this series of notebooks, we will \n",
    "1. Motivate the need for developing a rich theory of mathematical optimization\n",
    "2. How seemingly simple problems can lead to the necessity of more advanced optimization algorithms \n",
    "3. Discuss what techniques many modern commericial solvers (likely) use to achieve their state of the art performance \n",
    "\n",
    "Then, we will translate the ideas of these notebooks into well made YouTube videos.\n",
    "\n",
    "To give a flavor of this, we will start with a motivating algorithm: Gradient Descent (GD)\n",
    "\n",
    "In many machine learning contexts, we can use gradient descent to train models to do things like prediction and classification. But, gradient descent has many pitfalls, in part due to it's simplicity. However, it is still a very good spot to start to see why the theory of mathematical optimization is so powerful and necessary to allow the AI revolution to be as successful as it currently is. Tools like transformers and neural networks rely on optimization in order to be so effective. Additionally, many other fields, such as robotics, utilize optimziation for robotic planning, perception, and control.\n",
    "\n",
    "GD is one of the simplest algorithms out there to solve optimization problems, in part because of how intuitive it is. \n",
    "\n",
    "Consider a function $f: \\mathbb{R} \\to \\mathbb{R}$ such that $f(x) = x^2$. Here, gradient descent, starting at a random point, and a sufficient step size choice, will work quite well. \n",
    "\n",
    "Gradient Descent is the following algorithm. Essentially, think of a marble rolling down a hill.\n",
    "\n",
    "1. Choose $x \\in \\mathcal{D}$, where $\\mathcal{D}$ is the domain of your problem. In this case, we have $\\mathbb{R}$. Choose a step size $t > 0$. $t$ is typically chosen to be small. Here, suppose $t = \\frac{1}{4}$. \n",
    "2. Then, for $k = 1,...,$ until some *stopping criterion*, perform the following updates: \n",
    "    a. Compute $\\nabla f(x)$\n",
    "    b. Update $x := x - t \\dot \\nabla f(x)$\n",
    "\n",
    "And that's it! \n",
    "\n",
    "However, even a slightly more complicated problem $f: \\mathbb{R}^2 \\to \\mathbb{R}$ such that $f(x,y) = 2x^2-x^4+\\frac{x^6}{6}+x(y-1)+y^2$ (source: modified from https://www.sfu.ca/~ssurjano/camel3.html). This only a function of two variables, which is extremely small for ML problems.\n",
    "\n",
    "However, this function is small enough to demonstrate one major issue present with just applying a naive gradient descent. \n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "## Important visualizations for above \n",
    "1. Drawing functions using `manim`. \n",
    "2. animating GD on $x^2$ to show the marble rolling down the hill just fine \n",
    "3. Probably some logos and pictures / drawings of the famous / exiciting ML architectures. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8dd284-4f2c-484f-ac19-fe2e80a8a037",
   "metadata": {},
   "source": [
    "# The Problem of Nonconvexity \n",
    "- The big one. Why people care about optimization and relaxations and global techniques and such.\n",
    "\n",
    "From https://en.wikipedia.org/wiki/R._Tyrrell_Rockafellar , \"The great watershed in optimization is not between linearity and nonlinearity, but between convexity and nonconvexity\". https://x.com/lauraalbertphd/status/799699080699985920 \n",
    "\n",
    "Rockafellar is a genius mathematician who pioneered the investigation of convex analysis, which leads to convex optimzation. As the quote suggests, the difference in optimization being \"easy\" and being \"hard\" is this divide between convex and nonconvex functions.\n",
    "\n",
    "This is because convex functions have the extremely important property that local minima are global. This means that we can do *local* search, such as gradient descent, to obtain a global minimizer to our optimization problem. This is vital because a global search is impossible, as the real numbers are far too large to search in a feasible amount of time (in fact, it could take forever!).\n",
    "\n",
    "The function above has a main problem of being **nonconvex**. We can see this because of the three different minimizers. \n",
    "\n",
    "What would happen if we applied gradient descent? \n",
    "- If we started at some random point $x \\in \\mathcal{D}$, it is extremely unlikely we would find the global minima. Depending on the application area, this can be disasterous. \n",
    "        - Visual idea here: Realizations of gradient descent with random initial starting points. \n",
    "- As such, gradient descent on this problem, as it currently stands, will not be very effective. Is there anything we can do? \n",
    "        - For polynomial optimization, there is a family of techniques that involve something known as a ``relaxation'' that will allow us to solve this problem.\n",
    "\n",
    "Randomly applying gradient descent to this problem with a random initial point, which is a good scientific principle, will not lead to us finding the global minimum for generic problems. Therefore, it is critical to try to model problems as convex, or if we cannot, identify which precise part of our problem is causing nonconvexity. We can then ask ourselves what we lose by elminiating this nonconvexity for the sake of tractable solutions. Can we still interpret the solution for our context? Additionally, many problems reveal that elminiating nonconvexity does not even matter, and we can still solve the problem to global optimality despite ignoring the nonconvexity constraints (this is true for the convex relaxation of the shortest path problem, which will yield integer solutions despite optimizing over the reals).\n",
    "\n",
    "Nonconvex optimization is an active area of research. Some of the most popular strategies include the following principle of relaxation, which asks, \"what can do we do make this function 'as convex as possible', and how does the answer to that surrogate problem lead us to an optimizer of the original problem?\"\n",
    "\n",
    "**Forrest thought: This goes into the weeds of polynomial optimization, which involves some pretty complicated ideas and I think destracts from the main point of the video. I think what could be better here is to list some key phrases and buzzwords that could get folks interested in looking this up on their own?**\n",
    "\n",
    "- There is more problems of nonconvexity than just the objective function. What if the function is convex, but the domain is not? Consider the problem $\\min x^2$ subject to the constraint $1 - x^2 \\leq 1$. This is not a convex search space, since there are two directions we can choose. \n",
    "        - For this simple problem, we can see the solution. \n",
    "        - It turns out that there is a very powerful technique called the Lasserre hierarchy that allows us to solve this problem. This could be another video / a graduate class on polynomial optimziation.\n",
    "\n",
    "## Cool visuals for this section \n",
    "1. Picture of Rockafellar with a book cover from Convex Analysis probably. \n",
    "2. Visually performinga convex relaxation and showing how the minimizer to a particular convex relaxation (we will make it very clever) will be close to the minimizer for the actual problem. This can be done, and drawing it will just be a bit difficult but doable. This would be worth including if we want to discuss relaxation strategies. I think it could be worthwhile since this is how branch and bound works. But, that could mean its worhtwhile to save showing relaxations for a video on integer programming.\n",
    "3. Maybe some analytics / numbers about just how big optimization and the problem of nonconvexity is? Can look to INFORMS journals and such for this information I am sure. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c11d03-399c-436c-8d86-e978c6332c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f392cdb-7623-4992-bc5b-2412b25eb14b",
   "metadata": {},
   "source": [
    "# The problem of step size choice \n",
    "- Destroying convergence properties  \n",
    "\n",
    "The description of gradient descent above leaves a few very important questions that remain to be answered. \n",
    "1. What is a good stopping criterion? Essentially, when will know when to stop our gradient descent procedure? How do we know when we are at the bottom. \n",
    "2. What is a good choice of $t$? How do we know if it is too big or too small? Should we be adaptive? Should it decrease over time at a constant rate? \n",
    "\n",
    "\n",
    "### Stopping Criterion \n",
    "There are many stopping criterion we can consider. For our problems above, some simple ones may suffice. \n",
    "- Gradient is small  \n",
    "- Number of iterations \n",
    "- change in function is small \n",
    "\n",
    "More generally, good stopping conditions are informed from the KKT conditions, since these provide target equations that must be satisfied for a minimizer. And then, when we have a local minimizer, if our problem is convex, we can certify our solution as a global minimizer. \n",
    "\n",
    "### Choice of $t$ \n",
    "- Backtracking line search for \"sufficient decrease\"\n",
    "\n",
    "A constant step size will not work. Example on $x^2$. A constant decrease can be descent, but it won't be very effective as we approach the bottom. \n",
    "\n",
    "A good, technical one, is the from the backtracking line search. See Numerical Optimization and create a visualization using our function to show that this method is \"adaptive\" at finding a good step size. Additionally, theory tells us when this technique will work. \n",
    "\n",
    "\n",
    "## Cool visuals \n",
    "1. Visualizing the impact of these stopping criterions. This can be done on our simple problem of $x^2$, or the more complicated problem from the nonconvexity, showing that these stopping criterion will yield local minima. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d8bb7-5075-4957-b386-151e31b05b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "593c41fc-bd0f-45f2-96ad-474fd2156e88",
   "metadata": {},
   "source": [
    "# The problem of ill conditioning  \n",
    "- Making algorithms slow to glacial \n",
    "The contours can be an oval. For $f(x,y) = (x-1)^2 + 10(y-3)^2$ . This difference in numerical values can occur when some data is more impactful than others. \n",
    "\n",
    "Show the level sets. Since the gradient is *orthogonal*, we can make descent steps that are no where towards the minimum. \n",
    "\n",
    "Designing a preconditioner is an active field of research. Some examples are the Jacobian preconditioner. \n",
    "\n",
    "Ideally, we would want to use the \"inverse\" to change the shape of the level sets, and then recover the problem. \n",
    "\n",
    "The big problem with the \"inverse\" preconditioner design is that numerically, the inverse of these matrices can be impossible to actually compute in feasible time on a computer. Show a graph of this could be worthwhile. \n",
    "\n",
    "Then, to resolve this, we can get a solution vector from the preconditioned problem, and then recover the minimizer for the orignial problem simply. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1fb67-41f7-4fda-8d6f-2203e97ff701",
   "metadata": {},
   "source": [
    "## Looking beyond \n",
    "Newton's Method, trust region techniques (not line search), discrete optimization, riemannian optimization (optimization on manifolds), KKT Conditions, duality theory, polynomial optimization, branch and bound, linear programming, mixed integer programming, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f16e3-5677-413c-a23a-6b8a67140c3d",
   "metadata": {},
   "source": [
    "## Notes \n",
    "\n",
    "\n",
    "things to talk about \n",
    "1. nonconvexity and how that breaks this - the problem of local minima\n",
    "2. step size choice : backtracking line search or trust region methods \n",
    "3. Ill conditioning - making things glacial \n",
    "\n",
    "\n",
    "Can still be good for ML problems since it can avoid overfitting? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
